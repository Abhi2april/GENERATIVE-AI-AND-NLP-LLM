{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8820454,"sourceType":"datasetVersion","datasetId":5306365},{"sourceId":4298,"sourceType":"modelInstanceVersion","modelInstanceId":3093}],"dockerImageVersionId":30559,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction\n\n<img src=\"https://www.googleapis.com/download/storage/v1/b/kaggle-forum-message-attachments/o/inbox%2F769452%2Fb18d0513200d426e556b2b7b7c825981%2FRAG.png?generation=1695504022336680&alt=media\"></img>\n\n## Objective\n\nUse Llama 2.0, Langchain and ChromaDB to create a Retrieval Augmented Generation (RAG) system. This will allow us to ask questions about our documents (that were not included in the training data), without fine-tunning the Large Language Model (LLM).\nWhen using RAG, if you are given a question, you first do a retrieval step to fetch any relevant documents from a special database, a vector database where these documents were indexed. \n\n## Definitions\n\n* LLM - Large Language Model  \n* Llama 2.0 - LLM from Meta \n* Langchain - a framework designed to simplify the creation of applications using LLMs\n* Vector database - a database that organizes data through high-dimmensional vectors  \n* ChromaDB - vector database  \n* RAG - Retrieval Augmented Generation (see below more details about RAGs)\n\n## Model details\n\n* **Model**: Llama 2  \n* **Variation**: 7b-chat-hf  (7b: 7B dimm. hf: HuggingFace build)\n* **Version**: V1  \n* **Framework**: PyTorch  \n\nLlaMA 2 model is pretrained and fine-tuned with 2 Trillion tokens and 7 to 70 Billion parameters which makes it one of the powerful open source models. It is a highly improvement over LlaMA 1 model.\n\n\n## What is a Retrieval Augmented Generation (RAG) system?\n\nLarge Language Models (LLMs) has proven their ability to understand context and provide accurate answers to various NLP tasks, including summarization, Q&A, when prompted. While being able to provide very good answers to questions about information that they were trained with, they tend to hallucinate when the topic is about information that they do \"not know\", i.e. was not included in their training data. Retrieval Augmented Generation combines external resources with LLMs. The main two components of a RAG are therefore a retriever and a generator.  \n \nThe retriever part can be described as a system that is able to encode our data so that can be easily retrieved the relevant parts of it upon queriying it. The encoding is done using text embeddings, i.e. a model trained to create a vector representation of the information. The best option for implementing a retriever is a vector database. As vector database, there are multiple options, both open source or commercial products. Few examples are ChromaDB, Mevius, FAISS, Pinecone, Weaviate. Our option in this Notebook will be a local instance of ChromaDB (persistent).\n\nFor the generator part, the obvious option is a LLM. In this Notebook we will use a quantized LLaMA v2 model, from the Kaggle Models collection.  \n\nThe orchestration of the retriever and generator will be done using Langchain. A specialized function from Langchain allows us to create the receiver-generator in one line of code.\n\n## More about this  \n\nDo you want to learn more? Look into the `References` section for blog posts and in `More work on the same topic` for Notebooks about the technologies used here.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"# Introduction\n\n<img src=\"https://www.googleapis.com/download/storage/v1/b/kaggle-forum-message-attachments/o/inbox%2F769452%2Fb18d0513200d426e556b2b7b7c825981%2FRAG.png?generation=1695504022336680&alt=media\"></img>\n\n## Objective\n\nUse Llama 2.0, Langchain and ChromaDB to create a Retrieval Augmented Generation (RAG) system. This will allow us to ask questions about our documents (that were not included in the training data), without fine-tunning the Large Language Model (LLM).\nWhen using RAG, if you are given a question, you first do a retrieval step to fetch any relevant documents from a special database, a vector database where these documents were indexed. \n\n## Definitions\n\n* LLM - Large Language Model  \n* Llama 2.0 - LLM from Meta \n* Langchain - a framework designed to simplify the creation of applications using LLMs\n* Vector database - a database that organizes data through high-dimmensional vectors  \n* ChromaDB - vector database  \n* RAG - Retrieval Augmented Generation (see below more details about RAGs)\n\n## Model details\n\n* **Model**: Llama 2  \n* **Variation**: 7b-chat-hf  (7b: 7B dimm. hf: HuggingFace build)\n* **Version**: V1  \n* **Framework**: PyTorch  \n\nLlaMA 2 model is pretrained and fine-tuned with 2 Trillion tokens and 7 to 70 Billion parameters which makes it one of the powerful open source models. It is a highly improvement over LlaMA 1 model.\n\n\n## What is a Retrieval Augmented Generation (RAG) system?\n\nLarge Language Models (LLMs) has proven their ability to understand context and provide accurate answers to various NLP tasks, including summarization, Q&A, when prompted. While being able to provide very good answers to questions about information that they were trained with, they tend to hallucinate when the topic is about information that they do \"not know\", i.e. was not included in their training data. Retrieval Augmented Generation combines external resources with LLMs. The main two components of a RAG are therefore a retriever and a generator.  \n \nThe retriever part can be described as a system that is able to encode our data so that can be easily retrieved the relevant parts of it upon queriying it. The encoding is done using text embeddings, i.e. a model trained to create a vector representation of the information. The best option for implementing a retriever is a vector database. As vector database, there are multiple options, both open source or commercial products. Few examples are ChromaDB, Mevius, FAISS, Pinecone, Weaviate. Our option in this Notebook will be a local instance of ChromaDB (persistent).\n\nFor the generator part, the obvious option is a LLM. In this Notebook we will use a quantized LLaMA v2 model, from the Kaggle Models collection.  \n\nThe orchestration of the retriever and generator will be done using Langchain. A specialized function from Langchain allows us to create the receiver-generator in one line of code.\n\n## More about this  \n\nDo you want to learn more? Look into the `References` section for blog posts and in `More work on the same topic` for Notebooks about the technologies used here.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"# Installations, imports, utils","metadata":{}},{"cell_type":"code","source":"!pip install transformers==4.33.0 accelerate==0.22.0 einops==0.6.1 langchain==0.0.300 xformers==0.0.21 \\\nbitsandbytes==0.41.1 sentence_transformers==2.2.2 chromadb==0.4.12 --quiet","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-07-15T17:35:26.461147Z","iopub.execute_input":"2024-07-15T17:35:26.461524Z","iopub.status.idle":"2024-07-15T17:38:17.844049Z","shell.execute_reply.started":"2024-07-15T17:35:26.461491Z","shell.execute_reply":"2024-07-15T17:38:17.842920Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngoogle-cloud-pubsublite 1.8.2 requires overrides<7.0.0,>=6.0.1, but you have overrides 7.7.0 which is incompatible.\njupyterlab-lsp 4.2.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\ntorchdata 0.6.0 requires torch==2.0.0, but you have torch 2.0.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"from torch import cuda, bfloat16\nimport torch\nimport transformers\nfrom transformers import AutoTokenizer\nfrom time import time\n#import chromadb\n#from chromadb.config import Settings\nfrom langchain.llms import HuggingFacePipeline\nfrom langchain.document_loaders import TextLoader\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.embeddings import HuggingFaceEmbeddings\nfrom langchain.chains import RetrievalQA\nfrom langchain.vectorstores import Chroma\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-07-15T17:39:38.043568Z","iopub.execute_input":"2024-07-15T17:39:38.043929Z","iopub.status.idle":"2024-07-15T17:39:45.079915Z","shell.execute_reply.started":"2024-07-15T17:39:38.043902Z","shell.execute_reply":"2024-07-15T17:39:45.079158Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# Initialize model, tokenizer, query pipeline","metadata":{}},{"cell_type":"markdown","source":"Define the model, the device, and the `bitsandbytes` configuration.","metadata":{}},{"cell_type":"code","source":"model_id = '/kaggle/input/llama-2/pytorch/7b-chat-hf/1'\n\ndevice = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n\n# set quantization configuration to load large model with less GPU memory\n# this requires the `bitsandbytes` library\nbnb_config = transformers.BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type='nf4',\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_compute_dtype=bfloat16\n)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-07-15T17:40:09.041995Z","iopub.execute_input":"2024-07-15T17:40:09.042552Z","iopub.status.idle":"2024-07-15T17:40:09.103333Z","shell.execute_reply.started":"2024-07-15T17:40:09.042521Z","shell.execute_reply":"2024-07-15T17:40:09.102416Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"Prepare the model and the tokenizer.","metadata":{}},{"cell_type":"code","source":"time_1 = time()\nmodel_config = transformers.AutoConfig.from_pretrained(\n    model_id,\n)\nmodel = transformers.AutoModelForCausalLM.from_pretrained(\n    model_id,\n    trust_remote_code=True,\n    config=model_config,\n    quantization_config=bnb_config,\n    device_map='auto',\n)\ntokenizer = AutoTokenizer.from_pretrained(model_id)\ntime_2 = time()\nprint(f\"Prepare model, tokenizer: {round(time_2-time_1, 3)} sec.\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-07-15T17:40:18.387939Z","iopub.execute_input":"2024-07-15T17:40:18.388817Z","iopub.status.idle":"2024-07-15T17:44:05.563677Z","shell.execute_reply.started":"2024-07-15T17:40:18.388785Z","shell.execute_reply":"2024-07-15T17:44:05.562654Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6c7f7800183640868dcd9e4577ae68a0"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Prepare model, tokenizer: 227.17 sec.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Define the query pipeline.","metadata":{}},{"cell_type":"code","source":"time_1 = time()\nquery_pipeline = transformers.pipeline(\n        \"text-generation\",\n        model=model,\n        tokenizer=tokenizer,\n        torch_dtype=torch.float16,\n        device_map=\"auto\",)\ntime_2 = time()\nprint(f\"Prepare pipeline: {round(time_2-time_1, 3)} sec.\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-07-15T17:45:55.336350Z","iopub.execute_input":"2024-07-15T17:45:55.336722Z","iopub.status.idle":"2024-07-15T17:45:57.162914Z","shell.execute_reply.started":"2024-07-15T17:45:55.336694Z","shell.execute_reply":"2024-07-15T17:45:57.161900Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Prepare pipeline: 1.821 sec.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"We define a function for testing the pipeline.","metadata":{}},{"cell_type":"code","source":"def test_model(tokenizer, pipeline, prompt_to_test):\n    \"\"\"\n    Perform a query\n    print the result\n    Args:\n        tokenizer: the tokenizer\n        pipeline: the pipeline\n        prompt_to_test: the prompt\n    Returns\n        None\n    \"\"\"\n    # adapted from https://huggingface.co/blog/llama2#using-transformers\n    time_1 = time()\n    sequences = pipeline(\n        prompt_to_test,\n        do_sample=True,\n        top_k=10,\n        num_return_sequences=1,\n        eos_token_id=tokenizer.eos_token_id,\n        max_length=200,)\n    time_2 = time()\n    print(f\"Test inference: {round(time_2-time_1, 3)} sec.\")\n    for seq in sequences:\n        print(f\"Result: {seq['generated_text']}\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-07-15T17:46:00.561237Z","iopub.execute_input":"2024-07-15T17:46:00.562226Z","iopub.status.idle":"2024-07-15T17:46:00.569397Z","shell.execute_reply.started":"2024-07-15T17:46:00.562186Z","shell.execute_reply":"2024-07-15T17:46:00.568198Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"## Test the query pipeline\n\nWe test the pipeline with a query about the meaning of State of the Union (SOTU).","metadata":{}},{"cell_type":"code","source":"test_model(tokenizer,\n           query_pipeline,\n           \"Please explain what is the State of the Union address. Give just a definition. Keep it in 100 words.\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-07-15T17:46:04.712731Z","iopub.execute_input":"2024-07-15T17:46:04.713303Z","iopub.status.idle":"2024-07-15T17:46:12.282788Z","shell.execute_reply.started":"2024-07-15T17:46:04.713273Z","shell.execute_reply":"2024-07-15T17:46:12.281804Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1417: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation )\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Test inference: 7.565 sec.\nResult: Please explain what is the State of the Union address. Give just a definition. Keep it in 100 words.\n\nThe State of the Union address is an annual speech delivered by the President of the United States to a joint session of Congress, typically in January, in which the President reviews the current state of the union, highlights achievements and challenges, and proposes policy initiatives for the upcoming year. (Source: White House)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Retrieval Augmented Generation","metadata":{}},{"cell_type":"markdown","source":"## Check the model with a HuggingFace pipeline\n\n\nWe check the model with a HF pipeline, using a query about the meaning of State of the Union (SOTU).","metadata":{"execution":{"iopub.status.busy":"2023-09-23T19:22:16.433666Z","iopub.execute_input":"2023-09-23T19:22:16.434937Z","iopub.status.idle":"2023-09-23T19:22:16.440864Z","shell.execute_reply.started":"2023-09-23T19:22:16.434891Z","shell.execute_reply":"2023-09-23T19:22:16.439217Z"}}},{"cell_type":"code","source":"llm = HuggingFacePipeline(pipeline=query_pipeline)\n# checking again that everything is working fine\nllm(prompt=\"Please explain what is the State of the Union address. Give just a definition. Keep it in 100 words.\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-07-15T17:47:18.865470Z","iopub.execute_input":"2024-07-15T17:47:18.866188Z","iopub.status.idle":"2024-07-15T17:47:23.644800Z","shell.execute_reply.started":"2024-07-15T17:47:18.866138Z","shell.execute_reply":"2024-07-15T17:47:23.643884Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"'\\nThe State of the Union address is an annual speech given by the President of the United States to a joint session of Congress, in which the President reports on the current state of the union and outlines their legislative agenda for the upcoming year.'"},"metadata":{}}]},{"cell_type":"markdown","source":"## Ingestion of data using Text loder\n\nWe will ingest the newest presidential address, from Jan 2023.","metadata":{}},{"cell_type":"code","source":"!pip install langchain_community --quiet","metadata":{"execution":{"iopub.status.busy":"2024-07-15T17:47:40.383838Z","iopub.execute_input":"2024-07-15T17:47:40.384639Z","iopub.status.idle":"2024-07-15T17:47:58.522498Z","shell.execute_reply.started":"2024-07-15T17:47:40.384609Z","shell.execute_reply":"2024-07-15T17:47:58.521460Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.7 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 11.0.0 which is incompatible.\ncudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.2 which is incompatible.\ncudf 23.8.0 requires protobuf<5,>=4.21, but you have protobuf 3.20.3 which is incompatible.\ncuml 23.8.0 requires dask==2023.7.1, but you have dask 2023.9.0 which is incompatible.\ndask-cuda 23.8.0 requires dask==2023.7.1, but you have dask 2023.9.0 which is incompatible.\ndask-cuda 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.2 which is incompatible.\ndask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2023.9.0 which is incompatible.\ndask-cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.2 which is incompatible.\ndistributed 2023.7.1 requires dask==2023.7.1, but you have dask 2023.9.0 which is incompatible.\ngoogle-cloud-bigquery 2.34.4 requires packaging<22.0dev,>=14.3, but you have packaging 24.1 which is incompatible.\njupyterlab-lsp 4.2.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\nmomepy 0.6.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\npymc3 3.11.5 requires numpy<1.22.2,>=1.15.0, but you have numpy 1.23.5 which is incompatible.\npymc3 3.11.5 requires scipy<1.8.0,>=1.7.3, but you have scipy 1.11.2 which is incompatible.\nraft-dask 23.8.0 requires dask==2023.7.1, but you have dask 2023.9.0 which is incompatible.\nydata-profiling 4.3.1 requires scipy<1.11,>=1.4.1, but you have scipy 1.11.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"!pip install pymupdf --quiet","metadata":{"execution":{"iopub.status.busy":"2024-07-15T17:48:34.713006Z","iopub.execute_input":"2024-07-15T17:48:34.713390Z","iopub.status.idle":"2024-07-15T17:48:48.777957Z","shell.execute_reply.started":"2024-07-15T17:48:34.713356Z","shell.execute_reply":"2024-07-15T17:48:48.776813Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}]},{"cell_type":"code","source":"from langchain_community.document_loaders import PyMuPDFLoader","metadata":{"execution":{"iopub.status.busy":"2024-07-15T17:49:07.640838Z","iopub.execute_input":"2024-07-15T17:49:07.641512Z","iopub.status.idle":"2024-07-15T17:49:07.847090Z","shell.execute_reply.started":"2024-07-15T17:49:07.641477Z","shell.execute_reply":"2024-07-15T17:49:07.846313Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"loader = PyMuPDFLoader(\"/kaggle/input/9th-science/iesc104.pdf\")\ndocuments = loader.load()\n\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=256, chunk_overlap=20)\nall_splits = text_splitter.split_documents(documents)\n\nmodel_name = \"sentence-transformers/all-mpnet-base-v2\"\nmodel_kwargs = {\"device\": \"cuda\"}\nembeddings = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs)\n\nvectordb = Chroma.from_documents(documents=all_splits, embedding=embeddings, persist_directory=\"chroma_db\")\n\nretriever = vectordb.as_retriever()\nqa = RetrievalQA.from_chain_type(\n    llm=llm, \n    chain_type=\"stuff\", \n    retriever=retriever, \n    verbose=True\n)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-07-15T17:50:05.738004Z","iopub.execute_input":"2024-07-15T17:50:05.738651Z","iopub.status.idle":"2024-07-15T17:50:47.279659Z","shell.execute_reply.started":"2024-07-15T17:50:05.738620Z","shell.execute_reply":"2024-07-15T17:50:47.278580Z"},"trusted":true},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading .gitattributes:   0%|          | 0.00/1.23k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cdd76eb60a6f415d8561e0cbd466d53b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading 1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b562378ff6f744b894b20fc8201cc23d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading README.md:   0%|          | 0.00/10.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6e8a0ce839464b1696375c23a359a6bf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1d8452f727594487824709a9ace93d47"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)ce_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"54d6900aba2f495ab018ae29976064f3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data_config.json:   0%|          | 0.00/39.3k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c14d54f5d3724b2f8be61ec9df5f0581"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ea678ea360f5481eb39309e30c425380"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading pytorch_model.bin:   0%|          | 0.00/438M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d998a760d88c446c97fab5204d9a387b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)nce_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2d94883babc5483187aa10981c34fbbc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)cial_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"44757f1bcc184431a5933c30f1c5625b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b47db9667cdb4509a851b6e82ab63eb0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"daf6d8b6ed3e4f68824807a851a7c312"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading train_script.py:   0%|          | 0.00/13.1k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1bbe6082c79946998dd94f06a0492f2c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee86f4282aa7413fbc5d1a5882a5f733"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ffe718e909c42f4b8a19062a1ffd2b1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"023472bead874ff0bdfbbcd58a0b939b"}},"metadata":{}}]},{"cell_type":"code","source":"def test_rag(qa, query):\n    print(f\"Query: {query}\\n\")\n    time_1 = time()\n    result = qa.run(query)\n    time_2 = time()\n    print(f\"Inference time: {round(time_2-time_1, 3)} sec.\")\n    print(\"\\nResult: \", result)","metadata":{"execution":{"iopub.status.busy":"2024-07-15T17:58:47.735858Z","iopub.execute_input":"2024-07-15T17:58:47.736699Z","iopub.status.idle":"2024-07-15T17:58:47.742607Z","shell.execute_reply.started":"2024-07-15T17:58:47.736664Z","shell.execute_reply":"2024-07-15T17:58:47.741454Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"query=\"explain rutherfords model with dotted diagram\"\ntest_rag(qa, query)","metadata":{"execution":{"iopub.status.busy":"2024-07-15T18:02:33.115713Z","iopub.execute_input":"2024-07-15T18:02:33.116629Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Query: explain rutherfords model with dotted diagram\n\n\n\n\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4f0f61f8706046eeb59f06798b2f8ecb"}},"metadata":{}}]},{"cell_type":"code","source":"\n\nquery=\"what are the main topics covered the given data\"\ntest_rag(qa, query)\n\n\n\n\n\nquery = \"Generate 2 new questions from the uploaded data of a chapter to help me added in the data\"\ntest_rag(qa, query)\n\nquery = \"\"\"\nyou are assignned to generate 2 new questions i.e. 1 new theoritical based and 1 new numerical based question from the important topics of the uploaded pdf of a textbook's chapter.\"\"\"\ntest_rag(qa, query)\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport shutil\nshutil.rmtree(\"/kaggle/working/chroma_db\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"query = \"\"\"You are an intelligent assistant that analyzes and learns from the data on how questions are prepared in the provided documents. Your task is to generate new questions based on specific topics from the uploaded PDFs. From each important topic , Generate one new theoretical question and one new numerical question.\"\"\"\ntest_rag(qa, query)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#MATHEMATICS\n\nquery = \"\"\"\n Ensure that any problem you generate doesnot entirely match the given questions of the data(just change the digits),\nNow generate 2 new mathematics questions inspired from the topics of the given data of the provided chapter.\n\"\"\"\ntest_rag(qa, query)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import shutil\nshutil.rmtree(\"/kaggle/working/chroma_db\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loader = PyMuPDFLoader(\"/kaggle/input/8th-science/hesc112.pdf\")\n\ndocuments = loader.load()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"documents[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Split data in chunks\n\nWe split data in chunks using a recursive character text splitter.","metadata":{}},{"cell_type":"code","source":"text_splitter = RecursiveCharacterTextSplitter(chunk_size=256, chunk_overlap=20)\nall_splits = text_splitter.split_documents(documents)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Creating Embeddings and Storing in Vector Store","metadata":{}},{"cell_type":"markdown","source":"Create the embeddings using Sentence Transformer and HuggingFace embeddings.","metadata":{}},{"cell_type":"code","source":"model_name = \"sentence-transformers/all-mpnet-base-v2\"\nmodel_kwargs = {\"device\": \"cuda\"}\n\nembeddings = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Initialize ChromaDB with the document splits, the embeddings defined previously and with the option to persist it locally.","metadata":{}},{"cell_type":"code","source":"vectordb = Chroma.from_documents(documents=all_splits, embedding=embeddings, persist_directory=\"chroma_db\")","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Initialize chain","metadata":{}},{"cell_type":"code","source":"retriever = vectordb.as_retriever()\n\nqa = RetrievalQA.from_chain_type(\n    llm=llm, \n    chain_type=\"stuff\", \n    retriever=retriever, \n    verbose=True\n)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Test the Retrieval-Augmented Generation \n\n\nWe define a test function, that will run the query and time it.","metadata":{}},{"cell_type":"code","source":"def test_rag(qa, query):\n    print(f\"Query: {query}\\n\")\n    time_1 = time()\n    result = qa.run(query)\n    time_2 = time()\n    print(f\"Inference time: {round(time_2-time_1, 3)} sec.\")\n    print(\"\\nResult: \", result)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's check few queries.","metadata":{}},{"cell_type":"code","source":"query=\"what are the main topics covered the given data\"\ntest_rag(qa, query)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"Your task is to create original mathematics problems,\ninspired by the content and concepts covered in this chapter. The problems should be of similar difficulty and relevance to the 8th-grade level but must be distinct from any specific questions already present in the provided PDFs.\n, now please generate 2 questions from the pdf, make necessary changes so that the question becomes unidentical and not be exactly same.\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# SCIENCE OR P.C.M.\n\nquery = \"\"\"you are an intelligent assistant to assist educators to generate curricullam based question on the provideed pdf of class 8th science students\n, now you are assignned to generate 2 questions from the provided pdf of class 8th science textbook\"\"\"\ntest_rag(qa, query)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"query = \"\"\"\nyou are an intelligent assistant to assist educators to generate curricullam based question on the provideed pdf of class 8th mathematics\nInstructions for Generating Mathematics Problems:-\nTopic Coverage: Ensure that the problems you create span the various topics covered in this chapter.\nOriginality: The questions you generate should not be identical to or minor variations of the questions already present in the PDFs. Instead, use the concepts and examples as a foundation to create entirely new problems.\n\nNow generate 2 mathematics questions.\n\"\"\"\ntest_rag(qa, query)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"query = \"\"\"you are an intelligent assistant to assist educators to generate curricullam based question on the provideed pdf of class 8th mathematics\n, now you are assignned to generate 5 numerical questions from the provided pdf of class 8th maths textbook\"\"\"\ntest_rag(qa, query)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"query = \"\"\"you are an intelligent assistant to assist educators to generate curricullam based question on the provideed pdf of class 7th science students\n, now you are assignned to generate 5 multiple choice question from the provided pdf of class 7th science textbook \"\"\"\ntest_rag(qa, query)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"query = \"\"\"\nYou are an intelligent assistant to create original mathematics problems inspired by the questions and concepts covered in the provided pdf. Now generate 2 new mathematics questions\"\"\"\ntest_rag(qa, query)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import shutil\nshutil.rmtree(\"/kaggle/working/chroma_db\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Document sources\n\nLet's check the documents sources, for the last query run.","metadata":{}},{"cell_type":"markdown","source":"# Conclusions\n\n\nWe used Langchain, ChromaDB and Llama 2 as a LLM to build a Retrieval Augmented Generation solution. For testing, we were using the latest State of the Union address from Jan 2023.\n\n\n# More work on the same topic\n\nYou can find more details about how to use a LLM with Kaggle. Few interesting topics are treated in:  \n\n* https://www.kaggle.com/code/gpreda/test-llama-2-quantized-with-llama-cpp (quantizing LLama 2 model using llama.cpp)\n* https://www.kaggle.com/code/gpreda/fast-test-of-llama-v2-pre-quantized-with-llama-cpp  (quantized Llamam 2 model using llama.cpp)  \n* https://www.kaggle.com/code/gpreda/test-of-llama-2-quantized-with-llama-cpp-on-cpu (quantized model using llama.cpp - running on CPU)  \n* https://www.kaggle.com/code/gpreda/explore-enron-emails-with-langchain-and-llama-v2 (Explore Enron Emails with Langchain and Llama v2)\n","metadata":{}},{"cell_type":"markdown","source":"# References  \n\n[1] Murtuza Kazmi, Using LLaMA 2.0, FAISS and LangChain for Question-Answering on Your Own Data, https://medium.com/@murtuza753/using-llama-2-0-faiss-and-langchain-for-question-answering-on-your-own-data-682241488476  \n\n[2] Patrick Lewis, Ethan Perez, et. al., Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks, https://browse.arxiv.org/pdf/2005.11401.pdf \n\n[3] Minhajul Hoque, Retrieval Augmented Generation: Grounding AI Responses in Factual Data, https://medium.com/@minh.hoque/retrieval-augmented-generation-grounding-ai-responses-in-factual-data-b7855c059322  \n\n[4] Fangrui Liu\t, Discover the Performance Gain with Retrieval Augmented Generation, https://thenewstack.io/discover-the-performance-gain-with-retrieval-augmented-generation/\n\n[5] Andrew, How to use Retrieval-Augmented Generation (RAG) with Llama 2, https://agi-sphere.com/retrieval-augmented-generation-llama2/   \n\n[6] Yogendra Sisodia, Retrieval Augmented Generation Using Llama2 And Falcon, https://medium.com/@scholarly360/retrieval-augmented-generation-using-llama2-and-falcon-ed26c7b14670   \n\n","metadata":{}}]}